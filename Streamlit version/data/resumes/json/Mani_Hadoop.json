{
    "id": "resume_96348e1c-5df9-427b-a95a-fd64ca315851",
    "metadata": {
        "filename": "Mani_Hadoop.docx",
        "processed_date": "2025-09-24T08:45:43.756206+00:00"
    },
    "raw_text": "Mani\nSr. Hadoop Developer\nEmail: \tvmaniv08@gmail.com \t\t\t\t\t\t             Contact: (615) 813-1551\n\nPROFESSIONAL SUMMARY:\n\n8+ years of overall software development experience on Big Data Technologies, Hadoop Eco system and Java/J2EE Technologies with experience programming in Java, Scala, Python and SQL\n4+ years of strong hands-on experience on Hadoop Ecosystem including Spark, Map-Reduce, HIVE, Pig, HDFS, YARN, HBase, Oozie, Kafka, Sqoop, Flume.\nExperience in architecting, designing, and building distributed software systems. \nScala and Java, Created frameworks for processing data pipelines through Spark\nWrote python scripts to parse XML documents and load the data in database.\nDeep knowledge of troubleshooting and tuning Spark applications and Hive scripts to achieve optimal performance.\nUsed Sqoop to import data into HDFS / Hive from RDBMS and exporting data back to HDFS or HIVE from RDBMS.\nWorked with real-time data processing and streaming techniques using Spark streaming, Storm and Kafka.\nExperience developing Kafka producers and Kafka Consumers for streaming millions of events per second on streaming data\nSignificant experience writing custom UDF’s in Hive and custom Input Formats in MapReduce.\nKnowledge of job workflow scheduling and monitoring tools like Oozie.\nStrong experience productionalizing end to end data pipelines on hadoop platform.\nExpertise in Database Design, Creation and Management of Schemas, writing Stored Procedures, Functions, DDL and DML SQL queries and writing complex queries for Oracle\nExperience working with NoSQL database technologies, including MongoDB, Cassandra and HBase.\nGood experience is designing and implementing end to end Data Security and Governance within Hadoop Platform using Kerberos.\nStrong experience with UNIX shell scripts and commands.\nExperience in using various Hadoop Distributions like Cloudera, Hortonworks and Amazon EMR.\nStrong hands-on development experience with Java, J2EE (Servlets, JSP, Java Beans, EJB, JDBC, JMS, Web Services) and related technologies.\nWork with the team to help understand requirements, evaluate new features, architecture and help drive decisions.\nExcellent interpersonal, communication, problem solving and analytical skills with ability to make independent decisions\nExperience successfully delivering applications using agile methodologies including extreme programming, SCRUM and Test-Driven Development (TDD).\nExperience in Object Oriented Analysis, Design, and Programming of distributed web-based applications.\nExtensive experience in developing standalone multithreaded applications.\nConfigured and developed web applications in Spring and employed spring MVC architecture and Inversion of Control.\nExperience in building, deploying and integrating applications in Application Servers with ANT, Maven and Gradle.\nSignificant application development experience with REST Web Services, SOAP, WSDL, and XML.\n\nTECHNICAL SKILLS:\n\n\n\n\nPROFESSIONAL EXPERIENCE:\n\nClient\t\t: \tTMNAS\t\t\t\t\t\t        Sep 2016 – Present\nLocation\t: \tBala Cynwyd, PA\t\nRole\t\t:\tSr. Hadoop Developer\n\nProject Description: TMNA offers the security of nearby expertise, enhanced by the diversity and power of one of the world’s most respected insurance groups. Tokio Marine’s companies offer access to leading commercial insurance solutions spanning the property and casualty landscape including professional liability, workers’ compensation and property coverage.. The project deals with analyzing clickstream data of users who are visiting the company websites and applications to derive useful insights that help in optimizing future promotions and advertising.\n\nResponsibilities:\n\nInvolved in story-driven agile development methodology and actively participated in daily scrum meetings.\nIngested terabytes of click stream data from external systems like FTP Servers and S3 buckets into HDFS using custom Input Adaptors.\nImplemented end-to-end pipelines for performing user behavioral analytics to identify user-browsing patterns and provide rich experience and personalization to the visitors.\nDeveloped Kafka producers for streaming real-time clickstream events from external Rest services into topics.\nUsed HDFS File System API to connect to FTP Server and HDFS.  S3 AWS SDK for connecting to S3 buckets.\nWritten Scala based Spark applications for performing various data transformations, denormalization, and other custom processing.\nImplemented data pipeline using Spark, Hive, Sqoop and Kafka to ingest customer behavioral data into Hadoop platform to perform user behavioral analytics.\nCreated a multi-threaded Java application running on edge node for pulling the raw clickstream data from FTP servers and AWS S3 buckets.\nDeveloped Spark streaming jobs using Scala for real time processing.\nInvolved in creating external Hive tables from the files stored in the HDFS.\nOptimized the Hive tables utilizing improvement techniques like partitions and bucketing to give better execution Hive QL queries.\nUsed Spark-SQL to read data from hive tables, and perform various transformations like changing date format and breaking complex columns. \nWrote spark application to load the transformed data back into the Hive tables using parquet format.\nUsed Oozie Scheduler system to automate the pipeline workflow to exact data on a timely manner. \nImplemented installation and configuration of multi-node cluster on the cloud using Amazon \nWeb Services (AWS) on EC2.\nWorked on data visualization and analytics with research scientist and business stake holders.\n\nEnvironment: Hadoop 2.x, Spark, Scala, Hive, Pig, Sqoop, Oozie, Kafka, Cloudera Manager, Storm, ZooKeeper, HBase, Impala, YARN, Cassandra, JIRA, MySQL, Kerberos, Amazon AWS, Shell Scripting, SBT, Git, Maven.\n\n\nClient\t\t:\tDavita Inc\t\t\t    \t\t\t    Jan 2015 - Sep 2016 \nLocation\t:\tNashville, Tennessee\nRole\t\t:\tSr.Hadoop Developer\n\nProject Description: Davita is one of the largest kidney dialysis companies in world. The idea of the project is to ingest data from different multiple sources to Hadoop Data Lake, perform transformations on it according to business requirements and exporting the data to external systems. The system is a scalable BI platform that can adapt to the speed of the business by providing relevant, accessible, timely, connected, and accurate data.\n\nResponsibilities:\nInvolved in gathering and analyzing business requirements and designing Data Lake as per the requirements.\nBuilt distributed, scalable, and reliable data pipelines that ingest and process data at scale using Hive and MapReduce.\nDeveloped MapReduce jobs in Java for cleansing the data and preprocessing.\nLoaded transactional data from Teradata using Sqoop and create Hive Tables.\nExtensively used Sqoop for efficiently transferring bulk data between HDFS and relational databases.\nWorked on automation of delta feeds from Teradata using Sqoop and from FTP Servers to Hive.\nWorked on various performance optimizations like using distributed cache for small datasets, Partition, Bucketing in hive and Map Side joins.\nCreated components like Hive UDFs for missing functionality in HIVE for analytics.\nUsed IMPALA to analyze the data present in Hive tables.\nHandled Avro and JSON data in Hive using Hive SerDe.\nWorked with different compression codecs like GZIP, SNAPPY and BZIP2 in MapReduce, Pig and Hive for better performance.\nAnalyzed the data by performing the Hive queries using Hive QL to study the customer behavior.\nWrote python scripts to parse XML documents and load the data in database.\nGenerate auto mails by using Python scripts.\nImplemented the recurring workflows using Oozie to automate the scheduling flow.\nWorked with application teams to install OS level updates and version upgrades for Hadoop cluster environments.\nParticipated in design and code reviews.\n\nEnvironment: HDFS, Hadoop, Pig, Hive, HBase, Sqoop, Talend, Flume, Map Reduce, Podium Data, Oozie, Java 6/7, Oracle 10g, YARN, UNIX Shell Scripting, SOAP, REST services, Oracle 10g, Maven, Agile Methodology, JIRA.\n\nClient\t\t:\tNASBA\t\t\t\t\t\t\t    Aug 2012 - Dec 2014 \nLocation\t:\tNashville, TN\nRole\t\t:\tHadoop Developer\n\nProject Description: National Association of State Boards of Accountancy enhances the effectiveness and advance the common interests of the Boards of Accountancy. Existing ETL platform is overloaded with data coming from variety of sources and as data is growing day by day, it is not able to perform well and cost of managing the Relational database servers are going up. So, the data is migrated from multiple sources to Hadoop Data Lake and transformations are performed on it according to business requirements and the processed data is exported to external systems.\n\nResponsibilities: \nAnalysed business requirements and created/updated Software Requirements and design documents\t\nImported the data from relational databases to Hadoop cluster by using Sqoop. \nProvided batch processing solution to certain unstructured and large volume of data by using Hadoop Map Reduce framework.\nDeveloped data pipelines using Hive scripts to transform data from Teradata, DB2 data sources. These pipelines had customized UDF'S to extend the ETL functionality. \nDeveloped UDF for converting data from Hive table to JSON format as per client requirement. \nInvolved in creating tables in Hive and writing scripts and queries to load data into Hive tables from HDFS.\nImplemented dynamic partitioning and Bucketing in Hive as part of performance tuning. \nCreated custom UDF’s in Pig and Hive.\nPerformed various transformations on data like changing date patterns, converting to other time zones etc.\nDesigned and developed PIG Latin Scripts to process data in a batch to perform trend analysis. \nAutomated Sqoop, hive and pig jobs using Oozie scheduling. \nStoring, processing and analyzing huge data-set for getting valuable insights from them. \nCreated various aggregated datasets for easy and faster reporting using Tableau.\n\nEnvironment: HDFS, Map Reduce, Hive, Sqoop, Pig, HBase, Oozie, CDH distribution, Java, Eclipse, Shell Scripts, Tableau, Windows, Linux.\n\nClient\t\t:\tCopart Inc\t\t\t\t\t\t    Oct 2010 - Aug 2012 \nLocation\t:\tDallas, TX\nRole\t\t:\tJava Developer\n\nProject Description:  Copart makes it easy for Members to find, bid and win the vehicles that they are looking for. Members can choose from classics, early and late model cars and trucks, industrial vehicles and more. These internal applications are used for content management through which users can request a new site, which is used to store their project related documents.\n\nResponsibilities:\nDeveloped the J2EE application based on the Service Oriented Architecture by employing SOAP and other tools for data exchanges and updates.\nWorked in all the modules of the application which involved front-end presentation logic - developed using Spring MVC, JSP, JSTL and JavaScript, Business objects - developed using POJOs and data access layer - using Hibernate framework. \nDesigned the GUI of the application using JavaScript, HTML, CSS, Servlets, and JSP.\nInvolved in writing AJAX scripts for the requests to process quickly.\nUsed Dependency Injection feature and AOP features of Spring framework to handle exceptions.\nInvolved in writing Hibernate Query Language (HQL) for persistence layer.\nImplemented persistence layer using Hibernate that uses the POJOs to represent the persistence database.\nUsed JDBC to connect to backend databases, Oracle and SQL Server 2005.  \nProficient in writing SQL queries, stored procedures for multiple databases, Oracle and SQL Server.\nWrote backend jobs based on Core Java & Oracle Data Base to be run daily/weekly.\nUsed Restful API and SOAP web services for internal and external consumption.\nUsed Core Java concepts like Collections, Garbage Collection, Multithreading, OOPs concepts and APIs to do encryption and compression of incoming request to provide security.\nWritten and implemented test scripts to support Test driven development (TDD) and continuous integration.\n\nEnvironment: Java, JSP, HTML, CSS, Ubuntu Operating System, JavaScript, AJAX, Servlets, Struts, Hibernate, EJB (Session Beans), Log4J, WebSphere, JNDI, Oracle, Windows XP, LINUX, ANT, Eclipse.\n\n\nClient\t\t:\tAricent\t\t\t\t\t\t\t    Nov 2008 - Sep 2010    \nLocation\t:\tHyderabad, India\nRole\t\t:\tJava Developer\n\nProject Description:  Aricent is a global design and engineering company innovating in the digital era. help the world's leading companies solve their most important business and technology innovation challenges - from Customer to Chip. I have worked on developing the Aricent internal applications to automate the business process, store the documents. These internal applications are used for content management through which users can request a new site, which is used to store their project related documents.\n\nResponsibilities:\nAnalyzed user requirements and created Software Requirements and design documents\nResponsible for GUI development using Java, JSP, Struts\nDatabase design and development \nCreated and modified existing database scripts, Tables, Stored Procedures, and Triggers \nUsed XML functions, Cursors, Mail and Utility packages for Advanced search functionality\nCreated data correction and manipulation scripts for Production\nUsed JAXB for marshalling and un-marshalling of the data\nCreated JUnit tests for the service layer\nSupport for Production issues\nAttending the review meetings for scheduling, implementation and resolving issues in software development cycle\nEnvironment: Java, Struts, Java, Jsp, Servlets, JQuery, Ajax, XML, XSLT, JAXB, FOP, JBoss, Weblogic, Tomcat, SQL server 2005 and MyEclipse\n\n\nEDUCATION:\nBachelor of Technology in Computer Science from National Institute of Technology, Rourkela.               \n\n\n",
    "extracted_skills": [
        "agile",
        "aws",
        "cassandra",
        "communication",
        "css",
        "ec2",
        "git",
        "hadoop",
        "html",
        "java",
        "javascript",
        "jira",
        "mongodb",
        "mysql",
        "oracle",
        "problem solving",
        "python",
        "s3",
        "scala",
        "scrum",
        "spark",
        "sql",
        "tableau"
    ]
}