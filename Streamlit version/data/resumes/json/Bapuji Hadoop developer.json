{
    "id": "resume_76351da2-e678-4677-a79c-807c018a8aa4",
    "metadata": {
        "filename": "Bapuji Hadoop developer.docx",
        "processed_date": "2025-09-24T08:45:40.010343+00:00"
    },
    "raw_text": "Bapuji\nSr. Hadoop Developer\nPhone: +1(224)-706-0020   \nEmail: bapuji.dbj@gmail.com \n------------------------------------------------------------------------------------------------------------------------------------------\nPROFESSIONAL SUMMARY:\nOver 8+ years of experience including 4 years of Big Data Ecosystem related technologies with full project development, implementation and deployment.\nStrong Experience working with various Hadoop ecosystem components like, Map Reduce, HDFS, Hive, Sqoop, Pig, Flume, and Oozie.\nStrong Knowledge on Architecture of Distributed systems and Parallel processing frameworks. \nIn-depth understanding of MapReduce Framework and Spark execution model.\nWorked extensively on fine-tuning long running Spark Applications to utilize better parallelism and executor memory for more caching.\nStrong experience working with both batch and real-time processing using Spark framework.\nExpertise in developing production ready Spark applications utilizing Spark-Core, Data frames, Spark-SQL, Spark-ML and Spark-Streaming API's.\nHands on experience in installing, configuring and deploying Hadoop distributions in cloud environments (Amazon Web Services).\nExpertise in developing production ready Spark applications utilizing Spark-Core, Data frames, Spark-SQL, Spark-ML and Spark-Streaming API's.\nWorked on building real time data workflows using Kafka, Spark streaming and HBase. \nWorked extensively on Hive for building complex data analytical applications.\nVery good understanding of Partitions, bucketing concepts in Hive and designed both Managed and External tables in Hive to optimize performance.\nUsed custom serDes like Regex SerDe, JSON SerDe, CSV SerDe etc., in hive to handle multiple formats of data. \nHaving knowledge in Apache Ambari platform for securing, managing and monitoring Hadoop clusters.\nExperienced in Cluster coordination services through zookeeper.\nStrong experience using different columnar file formats like Avro, RCFile, ORC and Parquet formats.\nWorked with Sqoop to move (import/export) data from a relational database into Hadoop.\nExperience working with Hadoop clusters using Cloudera, Amazon EMR and Hortonworks distributions.\nExtensive experience in performing ETL on structured, semi-structured data using Pig Latin Scripts.\nDesigned and implemented Hive and Pig UDF's using Java for evaluation, filtering, loading and storing of data.\nExperienced in job workflow scheduling and monitoring tools like Oozie.\nWell versed with UNIX and Linux command line and shell script.\nAdequate knowledge and working experience with agile methodology.\n\nTECHNICAL SKILLS:\n\n\nEDUCATION:\nBachelor of Technology in Computer Science Engineering at JNTU, Kakinada, Andhra Pradesh, India.\n\nWORK EXPERIENCE:\n\nCigna – Bloomfield, Connecticut                                                             \t\t       Jul’17 – Present \nRole: Hadoop/Spark Developer \n\nResponsibilities:\nDeveloped Spark applications using Scala utilizing Data frames and Spark SQL API for faster processing of data.\nDeveloped highly optimized Spark applications to perform various data cleansing, validation, transformation and summarization activities according to the requirement\nData pipeline consists Spark, Hive and Sqoop and custom built Input Adapters to ingest, transform and analyze operational data.\nDeveloped Spark jobs and Hive Jobs to summarize and transform data.\nUsed Spark for interactive queries, processing of streaming data and integration with NoSQL database HBase, Cassandra for interactive access patterns.\nInvolved in converting Hive queries into Spark transformations using Spark Data Frames in Scala.\nAutomated creation and termination of AWS EMR clusters using AWS, java sdk.\nBuilt real time data pipelines by developing Kafka producers and spark streaming applications for consuming.\nIngested syslog messages to Kafka.\nWorked on Apache Airflow to schedule single and sometimes complex chains of tasks that depend on each other on regular intervals.\nHandled importing data from relational databases into HDFS using Sqoop and performing transformations using Hive and Spark.\nHaving knowledge in Apache Ambari platform for securing, managing and monitoring Hadoop clusters.\nExported the processed data to the relational databases using Sqoop, to further visualize and generate reports for the BI team.\nExperienced in cluster coordination services through Zookeeper.\nInstalled, tested and deployed monitoring solutions with Splunk services.\nUsed Hive to analyze the partitioned and bucketed data and computed various metrics for reporting.\nDeveloped Hive scripts in Hive QL to de-normalize and aggregate the data.\nScheduled and executed workflows in Oozie to run various jobs.\nDesigning & creating ETL jobs through Talend to load huge volumes of data into Cassandra, Hadoop Ecosystem and relational databases.\n\nEnvironment: Hadoop, Spark, Hive, Java, Scala, Maven, Impala, Oozie, Oracle, Ambari, GitHub, Tableau, Unix, Hortonworks, Apache Airflow Kafka, Zookeeper, Sqoop, Cassandra, Talend, Splunk, HBase.\n\nQualcomm -- San Diego, CA                                                                   \t\t          Dec’16 – Jun’17                                                                                            \nRole: Hadoop/Spark Developer\n\nResponsibilities:\n\nPart of Big Data Center of Excellence (CoE), responsible for designing and building enterprise data analytics platform.\nWorked with respective business units in understanding the scope of the analytics requirements.\nPerformed core ETL transformations in Spark.\nAutomated data pipelines which involve data ingestion, data cleansing, data preparation and data analytics.\nCreated end to end Spark applications using Scala to perform various data cleansing, validation, transformation and summarization activities on user behavioral data.\nDeveloped end-to-end data pipeline using FTP Adaptor, Spark, Hive and Impala.\nImplemented Spark utilizing Spark-SQL heavily for faster development, and processing of data.  \nExploring with Spark for improving the performance and optimization of the existing jobs in Hadoop using Spark-SQL, Data Frame running in Yarn mode.\nHandled importing other enterprise data from different data sources into HDFS using Sqoop and performing transformations using Hive, Map Reduce and then loading data into HBase tables.\nCollecting and aggregating large amounts of log data using Flume and staging data in HDFS for further analysis\nWrapper developed in Python for instantiating multithreaded application and running with other applications.\nAnalyzed the data by performing Hive queries (Hive QL) and running Pig scripts (Pig Latin) to study customer behavior.\nData warehousing, experience in design, development and testing, implementation and support of enterprise data warehouse.\nUsed Hive to analyze the partitioned and bucketed data and compute various metrics for reporting.\nCreated components like Hive UDFs for missing functionality in HIVE for analytics.\nWorked on various performance optimizations like using distributed cache for small datasets, Partition, Bucketing in Hive and Map Side joins.\nCreated Oozie workflows and coordinators to automate data pipelines daily, weekly and monthly.\nAutomated creation and termination of AWS EMR clusters using AWS, java sdk.\n\nEnvironment: AWS EMR, Hadoop, Spark, Hive, Sqoop, HBase, UNIX, Talend, Pig, Linux, Java, Scala, Python, Ambari, Zookeeper.\nHortonworks\n\nMcKesson - Alpharetta, GA                                                                   \t\t        Dec’15 – Nov’16\nHadoop/Spark Developer\n\nResponsibilities:\n\nDeveloped multithreaded Java based Input adaptors for ingesting click stream data from external sources like ftp server and S3 buckets on daily basis.\nCreated various spark applications using Scala to perform various enrichment of these click stream data combined with enterprise data of the users.\nImplemented batch processing of jobs using Spark Scala API.\nDeveloped Sqoop scripts to import/export data from Oracle to HDFS and into Hive tables. \nStored the data in columnar formats using Hive.\nInvolved building and managing NoSQL Database models using HBase.\nWorked in Spark to read the data from Hive and write it to Hbase.\nOptimized the Hive tables using optimization techniques like partitions and bucketing to provide better performance with Hive QL queries. \nWorked with multiple file formats like Avro, Sequence, Parquet and Orc.\nConverted existing MapReduce programs to Spark Applications for handling semi structured data like JSON files, Apache Log files, and other custom log data.\nLoaded the final processed data to HBase tables to allow downstream application team to build rich and data driven applications.\nWorked with a team to improve the performance and optimization of the existing algorithms in Hadoop using Spark, Spark -SQL, Data Frame.\nImplemented business logic in Hive and written UDF’s to process the data for analysis.\nUsed Oozie to define a workflow to coordinate the execution of Spark, Hive and Sqoop jobs.\nAddressing the issues occurring due to the huge volume of data and transitions.\nDesigned, documented operational problems by following standards and procedures using JIRA. \n\nEnvironment: Java, Hadoop 2.1.0, Map Reduce2, Spark, Unix, Pig 0.12.0, Hive 0.13.0, Linux, Sqoop 1.4.2, Flume 1.3.1, Eclipse, AWS EC2, and Cloudera CDH 4.  \n\nAmerican Home Shield - Memphis, TN                                                \t\t         Dec’14 – Nov’15\nRole: Hadoop Developer\n\nResponsibilities:\n\nMigrated the needed data from MySQL into HDFS using Sqoop and importing various formats of flat files in to HDFS.\nMainly worked on Hive queries to categorize data of different claims.\nInvolved in loading data from LINUX file system to HDFS\nWritten customized Hive UDFs in Java where the functionality is too complex.\nImplemented Partitioning, Dynamic Partitions, Buckets in HIVE.\nDesigning and creating Hive external tables using shared meta-store instead of derby with partitioning, dynamic partitioning and buckets.\nGenerate final reporting data using Tableau for testing by connecting to the corresponding Hive tables using Hive ODBC connector.\nResponsible to manage the test data coming from different sources\nReviewing peer table creation in Hive, data loading and queries.\nWeekly meetings with technical collaborators and active participation in code review sessions with senior and junior developers.\nMonitored System health and logs and respond accordingly to any warning or failure conditions.\nGained experience in managing and reviewing Hadoop log files.\nInvolved in scheduling Oozie workflow engine to run multiple Hive and pig jobs\nInvolved unit testing, interface testing, system testing and user acceptance testing of the workflow tool.\nCreated and maintained Technical documentation for launching Hadoop Clusters and for executing Hive queries and Pig Scripts\n\nEnvironment: Apache Hadoop, HDFS, Hive, Map Reduce, Core Java, Pig, Sqoop, Cloudera CDH4, Oracle, MySQL.\n\nProtective Life - Edina, MN                                                                      \t\t         Oct’13 - Nov’14     \nRole: Java Developer\n\nResponsibilities:\n\nImplemented a Web based Application using Servlets, JSP, spring, JDBC, XML. \nInvolved in writing Spring Configuration XML file that contains declarations and other   dependent objects declarations. \nUsed hibernate to connect to Database to create the DAO layer. \nDeveloped Application Framework using Model-View-Controller using the technology Spring. \nUsed HTML, XHTML, XML, XSLT, XPATH, JSP and Tag Libraries to develop view pages \nMultilayer Applications construction using Open JPA, HTML5, Spring MVC. \nAnnotated Spring Architecture (Spring Beans) \nImplemented UNIX shell scripts to migrate various data files to S&P ratings repository \nImplemented smooth pagination capability using JSP to remove existing pagination utility \nWorked on Geo API to provide geological access capability to S&P.com site. \nInvolved in Agile process to streamline development process with iterative development. \nCode reviews and Managing the CVS Repository. \nPrepare builds for DEV and UAT environments. \nParticipating in the regular team meetings sprint planning meetings, user story review meetings etc. \nInvolved in preparing High & low level design docs with UML diagrams using Microsoft VISIO tool. \n\nEnvironment: JDK 1.5, XML, HTML, XHTML, JSP, Spring, DAO, Oracle Express edition, Apache ANT, CVS, Junit, UNIX, Log4J, CSS Style Sheets, Apache Tomcat, J2EE, Maven 3\n\nAccenture – Hyderabad, India                                                                 \t\t       Oct’11– Sep’13\nRole: Java Developer\n\nResponsibilities:\n\nInvolved in Requirements analysis, design, and development and testing.\nInvolved in setting up the different roles & maintained authentication to the application.\nDesigned, deployed and tested Multi-tier application using the Java technologies.\nInvolved in front end development using JSP, HTML & CSS.\nImplemented the Application using Servlets\nDeployed the application on Oracle Web logic server\nImplemented Multithreading concepts in java classes to avoid deadlocking.\nUsed MySQL database to store data and execute SQL queries on the backend.\nPrepared and Maintained test environment.\nTested the application before going live to production.\nDocumented and communicated test result to the team lead on daily basis.\nInvolved in weekly meeting with team leads and manager to discuss the issues and status of the projects.\n\nEnvironment: J2EE (Java, JSP, JDBC, Multi-Threading), HTML, Oracle Web logic server, Eclipse, MySQL, JUnit.\n\nGolan Technologies – Hyderabad, India                                                \t\t          Jun’09 - Sep’11 \nRole: Java Developer\n\nGolan Technologies range from turnkey solutions to custom, client-driven solutions in a variety of product categories including website development and platform based applications, demand intelligence and business insight generation. Smart sites have the ability to provide a unified user experience and consistent messaging on websites across the globe, driving a favorable brand impression. \n\nResponsibilities:\n\nInvolved in the analysis, design, implementation, and testing of the project.\nDeveloped UI using HTML, JavaScript, CSS and JSP for interactive cross browser functionality and complex user interface.\nImplemented the end-to-end functionality of the client requirement during the development phase.\nImplemented the functionality of mapping entities to the database using Hibernate.\nWritten SQL queries involved in the JDBC connection in accordance with the business logic.\nPerformed various levels of unit testing for the entire application using the test cases, which included preparation of detail documentation for the results.\nActively participated in client meetings and taking the inputs for the additional functionality.\nInvolved in fixing bugs and unit testing with test cases using JUnit.\n\nEnvironment: J2EE, Spring, Hibernate, JavaScript, CSS, Servlets, MySQL\n\n",
    "extracted_skills": [
        "agile",
        "aws",
        "cassandra",
        "css",
        "ec2",
        "hadoop",
        "html",
        "java",
        "javascript",
        "jira",
        "mysql",
        "oracle",
        "python",
        "s3",
        "scala",
        "spark",
        "sql",
        "tableau"
    ]
}