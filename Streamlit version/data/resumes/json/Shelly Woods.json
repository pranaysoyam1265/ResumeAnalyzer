{
    "id": "resume_23a84ab2-000b-4117-a0e5-5a95e8dab639",
    "metadata": {
        "filename": "Shelly Woods.docx",
        "processed_date": "2025-09-24T08:45:54.007864+00:00"
    },
    "raw_text": "Shelly Woods\n(201) 203-3169\nShelly.woods@hrmediary.com                                                                                                                                                      \n\n  \nProfessional Summary\n\t\t\t\n\t\t\n10+ years of professional experience in IT on JAVA, JEE including 2+ years of hands on experience in Big Data, Hadoop Ecosystem Components.\nExpertise in developing application with financial domain, using Enterprise Technologies pertaining to  Core Java 1.8 , JEE , Servlets 2.2/2.3, JSP 2.0, Struts 2.0, Hibernate 3.0, Spring IOC, Spring MVC, Spring Boot Hibernate, JMS,XML, JDBC 2.0, JNDI, JAXP ,JAXB, Web Logic ,Web Sphere and Tomcat.\nExperience in Web Services using XML, HTML, SOAP and REST API.\nSolid background in Object Oriented Analysis & Design, Development and Implementation of Client Server/Web/Enterprise development using n-tier architecture\nknowledge of Angular JS practices ,Creating custom, general use modules and components which extend the elements and modules of core Angular JS\nStrong experience creating real time data streaming solutions using Apache Spark Core, Spark SQL & Data Frames, Spark Streaming \nIn depth knowledge of Hadoop Architecture and YARN\nExperience in writing Map Reduce programs using Apache Hadoop for analyzing Big Data.\nHands on experience in writing Ad-hoc Queries for moving data from HDFS to HIVE and analyzing the data using HIVE QL.\nExperience in importing and exporting data using Sqoop from Relational Database Systems to HDFS.\nExperience in writing Hadoop Jobs for analyzing data using Pig Latin.\nWorking Knowledge in NoSQL Databases like HBase.\nIntegrated Apache Kafka for data ingestion\nExperience in using Apache Flume for collecting, aggregating and moving large amounts of data from application servers.\nExperience in using Zookeeper and Oozie Operational Services for coordinating the cluster and scheduling workflows.\nExtensive experience with SQL, PL/SQL, Shell Scripting and database concepts.\nExperience in using version control management tools like CVS, SVN and Rational Clear Case.\nHighly motivated, self-starter with a positive attitude, willingness to learn new concepts and acceptance of challenges.\nAbility to work independently and with a group of peers in a results-driven environment. Strong analytical and problem solving skills. Ability to take initiative and learn emerging technologies and programming languages\n\nEducation:\n\nM.C.A (Master of Computer Applications) from Osmania University, 2006.\nB.S.C (Computer Science) from Kakatiya University, 2003.\n\n\n\nTechnical Skills:\t\t\n\t\t\n\n\nProfessional Experience:\n\n\nClient: \tAT&T, USA\t\t                                                                                               April 2017-Persent.\t\nTitle: Senior Analytical Hadoop / Spark Developer\n\nECOMP is critical in achieving AT&T’s D2 imperatives to increase the value of our network to customers by rapidly on-boarding new services (created by AT&T or 3rd parties), enabling the creation of a new ecosystem of cloud consumer and enterprise services, reducing Capital and Operational Expenditures, and providing Operations efficiencies. It delivers enhanced customer experience by allowing them in near real time to reconfigure their network, services, and capacity. While ECOMP does not directly support legacy physical elements, it works with traditional OSS’s to provide a seamless customer experience across both virtual and physical elements. ECOMP enables network agility, elasticity, and improves Time-to-Market/Revenue/Scale via the AT&T Service Design and Creation (ASDC) visual modeling and design. The service design and creation capabilities and policy recipes eliminate many of the manual and long running processes performed via traditional OSS’s (e.g., break-fix largely moves to plan and build function). The ECOMP platform provides external applications (OSS/BSS, customer apps, and 3rd party integration) with a secured, RESTful API access control to ECOMP services.\n\nResponsibilities:\nDeveloped Spark scripts by using Scala shell commands as per the requirement.\nUsed Spark API over Cloudera Hadoop YARN to perform analytics on data in Hive.\nDeveloped Scala scripts, UDFFs using both Data frames/SQL and RDD/MapReduce in Spark  for Data Aggregation, queries and writing data back into OLTP system through Sqoop.\nExperienced in performance tuning of Spark Applications for setting right Batch Interval time, correct level of Parallelism and memory tuning.\nLoaded the data into Spark RDD and do in memory data Computation to generate the Output response.\nOptimizing of existing algorithms in Hadoop using Spark Context, Spark-SQL, Data Frames and Pair RDD's.\nInvolved in loading data from Oracle database into HDFS using Sqoop queries.\nResponsible for building scalable distributed data solutions using Hadoop.\nDeveloped Map Reduce pipeline jobs to process the data and create necessary HFiles. \nDeveloped Pig Latin scripts for data cleansing.\n\nEnvironment: HDFS, Hive, HBase, Spark Core, Spark SQL, Spark Streaming, Scala, SBT 0.13, Oozie, kafka-0.9.0, Apache NiFi ,Java (jdk1.7 & 1.8 ), UNIX, SVN and Zookeeper, JEE, JSP, JSTL,  Spring 2.5, Oracle 11g/10g,Maven, REST-ful Web Services,  Apache Axis2, LINUX,  Tomcat7,GIt,Jenikins.\n\nClient: \tAmerican Express, USA\t\t                               August 2016-March2017.\t\nTitle: Senior Java/Hadoop Developer\n\nProject Description:\nAmerican Express Company is a global travel, financial and network service provider. The company provides individuals with charge and credit cards. In CCSG we build new application pages for the given cards (Charge/Credit). Enables prospect to apply for personal cards. Prospect has an option to apply for a personal card through various channels.\nLong Application: User is not an American Express card holder and wants to apply for a new card.\nShort Application: If already a card member, has an option to apply for new cards, with a difference that user is asked to fill in minimal details\n      \nResponsibilities: \nInvolved in writing MAVEN project, primary responsibilities include development of web application using spring MVC and Hibernate to pull queries from oracle\nInvolved in loading data from Oracle database into HDFS using Sqoop queries.\nResponsible for building scalable distributed data solutions using Hadoop.\nDeveloped Map Reduce pipeline jobs to process the data and create necessary HFiles. \nDeveloped Pig Latin scripts for data cleansing.\nWorked with different File Formats like TEXTFILE, AVROFILE for HIVE querying and processing.\n Developed PIG UDF'S for manipulating the data according to Business Requirements and also worked on developing custom PIG Loaders.\nI have successfully written Spark Streaming application to read streaming twitter data and analyze twitter records in real time using Yardstick framework to measure performance of Apache Ignite Streaming and Apache Spark Streaming. \nUsing Apache Nifi to Stream data Feeds to Kafka.\nInvolved in writing MAVEN project, primary responsibilities include development of web application using spring MVC and Hibernate to pull queries from oracle.\n\nImplemented test cases for Spark using Scala as language. \n\nEnvironment: Hadoop, Map Reduce, HDFS, Hive, HBase, Spark Core, Spark SQL, Spark Streaming, Scala, SBT 0.13, Oozie, kafka-0.9.0, Apache NiFi ,Java (jdk1.7 & 1.8 ), UNIX, SVN and Zookeeper, JEE, JSP, JSTL,  Spring 2.5, Oracle 11g/10g,Maven, REST-ful Web Services, SOAP, Apache Axis2, LINUX,  Tomcat7\n\t\nClient: \tUnited Overseas Bank, Singapore\t\t\t\tSeptember 2014- August 2016\nTitle:   Tech Lead( Java/Hadoop)\n\nDescription: \t         \n\nUOB, The Power Lender CE is a loan origination system supporting all kinds of loans. The Main objective of this project is to build an interface to interact with External Fraud Detection System for both Secured and Unsecured Loans. This system consists of two basic modules such as Secured Loans are mortgage loans, housing loans or any loan that is availed form the bank on providing some security or collateral. Unsecured Loans are credit cards, personal loans, vehicle loans or any loan that is availed from the bank without any security. \n\nResponsibilities:\nCollaborated with the Business Intelligence team to understand the high level data roadmap and define data discovery priorities.\nInstalled and configured Hadoop-1.0.2 Map Reduce, HDFS, and developed multiple Map Reduce jobs in Java for data cleaning and preprocessing. \nAnalyzed Hadoop cluster using the big data analytical tools such as Pig, Hive, Scoop and Flume. \nCollected and aggregated large amounts of web log data from different sources such as web servers, mobile and network devices using Apache Flume and stored the data into HDFS for analysis.\nDeveloped optimal strategies for distributing the web log data over the cluster, importing and exporting the stored web log data into HDFS and Hive using Scoop.\nInvolved in creating Hive tables, loading millions of records of the stored log data and writing queries that will invoke and run the Map Reduce jobs in the backend. \nTransformed large sets of semi-structured and unstructured data in various formats, to extract parameters such as user location, age, spending time etc.\nAnalyzed the web log data using Hive to calculate metrics such as number of unique visitors, page views, etc.\nExported the analyzed data to relational databases using Sqoop for visualization and generating reports.\nDesigned efficient high-performing applications to extract, transform, load, and query very large datasets, including unstructured data. \nInstalled Apache Oozie workflow engine to run multiple Hive and Pig jobs independently with time and data availability.\nModelled user behavior based upon previous findings and most relevant data available, and contributed to the development of tools for tracking and understanding user behavior.\nWorked on Hive joins to produce the input data set.\n\n\nEnvironment: Hadoop, Map Reduce, HDFS, Hive, Oracle 11g/10g, HBase, Spark Core, Spark SQL, Spark Streaming, Scala, SBT  0.13 ,Oozie, Java (jdk1.6), UNIX, SVN and Zookeeper,Java, J2EE, JSP, JSTL, AngularJS, Spring 2.5,Maven, REST-ful Web Services, SOAP, Apache Axis2, LINUX,  Tomcat7,Maven\n\nUniversal Weather & Aviation.                                        July 2010 - August 2014\t \nTitle: Senior Java Developer\n\nDescription:\t          \nThe application will provide enhanced electronic versions of Universal's product UVTripPlanner. It will provide search facility for ground services and airport data. This is the web application provides information about weather and aviation details for registered users across the globe. User can get the data to every single airport includes airport address, frequencies, customs, and services available for different types of aircrafts.  \n\n\nResponsibilities: \nUsed Agile Methodologies to manage full life-cycle development of the project.\nDeveloped application using Struts, spring and Hibernate.\nDeveloped rich user interface using JavaScript, JSTL, CSS, JQuery and JSP’s.\nDeveloped custom tags for implementing logic in JSP’s.\nUsed Java script, JQuery, JSTL, CSS and Struts 2 tags for developing the JSP’S.\nInvolved in making release builds for deploying the application for test environments.\nUsed Oracle database as backend database.\nWrote SQL to update and create database tables.\nUsed Eclipse as IDE.\nUsing RIDC Interface get content details and Create Content through application.\nUsed Spring IOC for injecting the beans.\nUsed Hibernate for connecting to the database and mapping the entities by using hibernate annotations.\nCreated JUnit test cases for unit testing application.\nWriting/integration JSP communicating to spring controller  and passing query criteria to hibernate to pull data  and showing reports based on searches both on web gui and streaming data into excel .\nInvolved in writing MAVEN project, primary responsibilities include development of web application using spring MVC and Hibernate to pull queries from oracle.\nDevelopment Tools are Maven, Spring MVC,Hibernate\nAppserversIwebservers:Tomcat7\nUsed JUNIT and JMOCK for unit testing.\n\nEnvironment: J2EE1.6, JSP, JSTL, Ajax, Spring 2.5, Struts 2.0, Ajax, Hibernate 3.2,JDBC, JNDI,XML, XSLT, Web Services, WSDL, Log4j, ORACLE 11g, Oracle Web logic Server 10.3, SVN, Windows XP, UML.\n\n\nHoneywell                                                                                                 December 2009- July 2010\nJava Developer\n\nDescription:     The Honeywell_AP process covers processing of all vendor related payments for clients, which includes processing of invoices, credit notes and payment requests etc. On an overall basis, timely and accurate payments with complete controllership and diligent customer service are the required deliverables of AP\n\nResponsibilities: \nInvolved in the requirements gathering. Design, Development, Unit testing and Bug fixing.\nUsed Agile Methodologies to manage full life-cycle development of the project.\nInvolved in development of GUI for Client Using Java Swing Development using Java1.6, Java swing worker also developed the entire online help for the application system using the Java help system, the application had perspectives for views to support multiple tab Components in the UI, it was packaged as a web start build.\nInvolved in creating ant scripts for the Jnlp files to update client builds for the Usability tests.\nWorked with Generics using Java1.6 and some other open source tools to build the C2C1M Client like, Table Layout, Jakarta P01etc, Worked with JDIC (Java Desktop integration Components) components to embed a web browser displaying HTML in a Java window.\nWorked on JMS to publish and subscribe to topics messages downstream to update legacy data using IBM MQ   series. \nCreated/Configured Hibernate mapping classes, 1-Hibernate configuration files (XML files) to use the Hibernate frame work to update or view Database records, also created Criteria Queries to retrieve data from the Database.\nDeveloped RMT servers of client and server could communicate for data updates and retrievals. The combination of hibernate with RMI was simple and perfect solution for concurrency issues.\nSoftware Development Methodology used to develop the project was Agile Methodology (with small scrum teams).Connected client to web service (JAX—RPC) to retrieve data and d display for central storage, the application was developed over (lie spring framework.)\nWorked on writing/updating ANT scripts while creating web archive files/enterprise archive files the jars were signed to maintain security for the systems.\nBuilt high performance java servers and used maven for building/packaging.\nDesign of the Client side of the application using Java swing (1.6) using the JSR296 application framework.\nCollaborating with analysis team to review the developed UI and packaging/deployment the application for UAT and SIT using java web start build techniques.\nDevelopment tools: JavaSwing,Intellijidea,YourKit Profiler \nApplication server: Websphere\nServer technology: RMI Remote Method Invocation), Hibernate. Version Control;Clearcase\nSpecial tools; JDIC, Table Layout (opensourceswing’s layout), Glazed Lists, lntelliJ IDEA, Your Kit Java Profiler (to clear memory leaks in the client/server code), Deadlocks P0I (Apache for exporting to excel from the GUI tables), hibernate..\n\nEnvironment: J2EE1.6, JSP, JSTL, Ajax, Spring 2.5, Struts 2.0, Ajax, Hibernate 3.2,JDBC, JNDI,XML, XSLT, Web Services, WSDL, Log4j, ORACLE 11g, Oracle Web logic Server 10.3, SVN, Windows XP, UML, Sterling Commerce Distributed Order Management(DOM).\n\n\nSpecialty Planners Inc                                                                           November 2007- December 2009\nJava/J2EE Developer\n\nDescription:  \nSpecialty Planners Inc. is an Insurance Agency sales company that sells Long Term Care (LTC) products of various Insurance companies (called Carriers) and gets commission for the business done, from the Carriers. Agents are recruited and trained by Specialty Planners to do business for the Carriers and the Carrier takes agent into contract. Specialty Planners supplies leads (prospect customer information) to its Agents via AIMS IC online application for doing business. The main objective of the AIMS application is that provides support and management for the Specialty Planners business. The system will allow Specialty Planners to operate efficiently and support brokerage business model and third party administration business.\n\n\nResponsibilities:\nInvolved in Unit Testing of the Application.\nDeveloped the JSP’s and Modeling data using MVC architecture.\nInvolved in designing front-end screens.\nDocumenting daily weekly status report and sending to client\nImplemented the Servlets and JSP components\nUsed Hibernate for Object Relational Mapping and data persistence.\n Developed the Database interaction classes using JDBC.\nCreated JUnit test cases and ANT scripts for build automation.\n\nEnvironment: Java, J2EE 1.4, HTML, XML, JDBC, JMS,  Servlets, JSP 1.2, Struts 1.2, Hibernate, Web services, Eclipse 3.3, Web Sphere 7,  Oracle 9i, ANT, Microsoft Visio.\n\n\n\n",
    "extracted_skills": [
        "agile",
        "angular",
        "css",
        "git",
        "hadoop",
        "html",
        "java",
        "javascript",
        "oracle",
        "problem solving",
        "rest api",
        "scala",
        "scrum",
        "spark",
        "sql"
    ]
}