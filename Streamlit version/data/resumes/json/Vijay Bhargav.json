{
    "id": "resume_3367942d-45f2-4a6e-9faf-e2daed31c57b",
    "metadata": {
        "filename": "Vijay Bhargav.docx",
        "processed_date": "2025-09-24T08:45:59.013741+00:00"
    },
    "raw_text": "Vijay\nSr. Java/ Hadoop Developer\n@:b.vijay.m09@gmail.com                                                                                   ©: (646) 801-5340\n\nResponsibility:\n9+ years of experience in SDLC with key emphasis on the trending Big Data and Java Technologies - Spark, Scala, Spark Mlib, Hadoop, Tableau, Cassandra, Java, J2EE.\nArchitect, design & develop Big Data Solutions practice including set up Big Data roadmap, build supporting infrastructure and team to provide Big Data.\nArchitecting, Solutioning and Modeling DI (Data Integrity) Platforms using sqoop, flume, kafka, Spark Streaming, Spark Mllib, Cassandra.\nStrong experience in migrating data warehouses and databases into Hadoop/NoSQL platforms.\nStrong expertise on Amazon AWS EC2, Dynamo DB, S3, Kinesis and other services \nExpertise in data analysis, design and modeling using tools like ErWin.\nExpertise in Big Data architecture like hadoop (Azure, Hortonworks, Cloudera) distributed system, MongoDB, NoSQL.\nExpertise in Service Oriented Architectures (SOA- Web Services) using Apache Axis, WebLogic, JBoss and EJB Web service framework. \nUsed Mule ESB in designing the application as a middleware between the third-party system and the customer side system.\nHands on experience on Hadoop /Big Data related technology experience in Storage, Querying, Processing and analysis of data. \nExpertise in archtiecting Big data solutions using Data ingestion, Data Storage\nStrong Experience in Front End Technologies like JSP, HTML5, JQuery, JavaScript, CSS3. \nWorked on windows server AD configuration and Kerberos protocol. \nExperienced with Perl, Shell scripting and test automation tools like Selenium RC, Web Driver and Selenium Grid.  \nDeveloped Python Mapper and Reducer scripts and implemented them using Hadoop streaming. \nExperienced in customizing Selenium API to suit in testing environment.  \nIntegration of Mule ESB system while utilizing MQ Series, Http, File system and SFTP transports.\nSolid Knowledge of My SQL and Oracle databases and writing SQL Queries. \nProficient in developing the application using JSF, Hibernate, Core Java, JDBC and Groovy and Grails presentation layer components using JSPs, Java script, XML and HTML Cassandra , Cucumber, OLE and Continuous deployment,  API , Angular JS along with Web service , REST , GemFire , Rabbit MQ , Spring Boot.. \nExperience in Back End Development including Web services, Data service layers with service desk experience.\nDesigned and coded Hibernate, struts for mapping, configurations and HQL for enhancement and new module development of Transport Optimization, Planning and Scheduling Web app. \nUsed Groovy and Grails with spring, Java, J2EE for user interface.\nInitiated the Automation framework using Selenium Web Driver to run test cases in multiple browsers and platforms.\nHighly motivated software engineer and experience in developing in web applications using Java script, Backbone.js and Coffee script technologies.   \nUtilized integration Patterns, integration tools, EAI, Transformations, XML Schemas, and XSLT. \nUsed Quartz connector to schedule the batch jobs. \nArchitected Integrations using Mule Soft ESB environments for both on premise and Cloud hub environments. \n• Experience in developing interfaces between Sales force and Oracle ERP using Informatica Cloud/Mule ESB technologies. \n• Implemented flows for sales force outbound / inbound calls and business process. \n• Experience in Mulesoft Any point API platform on designing and implementing Mule APIs.\nGood knowledge on Soap UI tool to unit testing SOA based applications. \nAbility to understand and use design patterns in application development. \nVery good knowledge in different development methodologies like SDLC and Agile. \nExperienced in developing applications using HIBERNATE (Object/Relational mapping framework) and involves in working on service desk client.\nExperienced in developing Web Services using JAX-RPC, JAXP, SOAP and WSDL. Also knowledgeable in using WSIF (Web Services Invocation Framework) API. \nExperience in writing database objects like Stored Procedures, Triggers, SQL, PL/SQL packages and Cursors for Oracle, SQL Server, DB2 and Sybase. \nJava 8, J2EE, spring (MVC, Data-JPA, Security),Hibernate, Jenkins or Bamboo, HTML 5, JSP, JavaScript, JQuery, Ajax, Angular JS.\nTechnical Skills\n\nBig Data: Hive, Hadoop, oozie, sqoop, Storm, Kafka, Elastic Search, HDFS, Zoo Keeper, Map Reduce, hive, pig, spark, flume.\nJ2EE Technologies: Servlets, JSP, JDBC, JNDI, OSGI, EJB, RMI, ASP.\nProgramming Languages:  Java 8, C, C++, Pig Latin, HQL, R, Python, XPath, Spark.\nFrameworks: Jakarta Struts, Spring, Spring MVC, JSF (Java Server Faces), Hibernate, Tiles, I Batis, Validator, Cucumber, OLE and Continuous deployment, micro services, Groovy.\nWeb Technologies: HTML, DHTML, Cassandra , API , Angular JS along with Web service, REST, Gem Fire, Rabbit MQ , Java script with J query, Python, Ext JS, AJAX, CSS,CMS, Yahoo UI, ice faces API , Angular, Node.js, Backbone.js.\nXML Technologies: XML 1.0, XSLT, XSL, HTML5, DHTML, J query,, XSL / XSLT /XSL-FO, JNDI, LDAP, SOAP, AXIS 2 \nApplication/Web Servers: IBM Web Sphere 5.X/6.0/7.0/8.0, IBM HTTP server 8.x, Web Logic 7.x/8.x/9.0, Web Logic Portal 5.x, J Boss 4.0, j BPM, Apache Tomcat, OC4J, Docker.\nNO SQL Data Base: Cassandra, mongo DB\nDatabases: Oracle 12c /10g/11g, SQL Server, My SQL, DB2. \nMessaging Systems: JMS, IBM MQ-Series \nIDE Tools: IBM Web Sphere Studio Application Developer (WSAD) RSA, RAD, Eclipse /RCP, J developer, Net Beans .\n\nProfessional Experience\n\nCigna, Hartford, CT\t\t\t                                                                              Aug’15 – Till Date\nSr. Java/Hadoop Developer\n      Roles & Responsibilities:\nImplementation of Big Data ecosystem (Hive, Impala, Sqoop, Flume, Spark, Lambda) with Cloud Architecture \nUsed Talend for Big data Integration using Spark and Hadoop\nUsed Microsoft Windows server and authenticated client server relationship via Kerbros protocol.\nExperience on BI reporting with At Scale OLAP for Big Data.\nImplemented solutions for ingesting data from various sources and processing the Data-at-Rest utilizing Big Data technologies such as Hadoop, Map Reduce Frameworks, HBase, Hive\nLoaded and transformed large sets of structured, semi structured and unstructured data using Hadoop/Big Data concepts. \nI have Working experience in Middleware Integration product Mulesoft \nDesigned and Developed Real time Stream processing Application using Spark, Kafka, Scala and Hive to perform Streaming ETL and apply Machine Learning.\nIdentify query duplication, complexity and dependency to minimize migration efforts \nTechnology stack: Oracle, Hortonworks HDP cluster, Attunity Visibility, Cloudera Navigator Optimizer, AWS Cloud and Dynamo DB.\nExperience in AWS, implementing solutions using services like (EC2, S3, RDS, Redshift, VPC)\nWorked on Talend Magic Quadrant for performing fast integration tasks.\nWorked as a Hadoop consultant on (Map Reduce/Pig/HIVE/Sqoop).\nWorked with Spark and Python.\nWorked using Apache Hadoop ecosystem components like HDFS, Hive, Sqoop, Pig, and Map Reduce.\nLead architecture and design of data processing, warehousing and analytics initiatives.\nWorked with AWS to implement the client-side encryption as Dynamo DB does not support at rest encryption at this time. \nExploring with the Spark for improving the performance and optimization of the existing algorithms in Hadoop using Spark Context, Spark-SQL, Data Frame, Pair RDD's, Spark YARN. \nUsed Data Frame API in Scala for converting the distributed collection of data organized into named columns. \nPerformed data profiling and transformation on the raw data using Pig, Python, and Java.\nExperienced with batch processing of data sources using Apache Spark. \nDeveloping predictive analytic using Apache Spark Scala APIs. \nInvolved in working of big data analysis using Pig and User defined functions (UDF).\nCreated Hive External tables and loaded the data into tables and query data using HQL.\nUsed Sqoop to efficiently transfer data between databases and HDFS and used Flume to stream the log data from servers. \nInvolved in developing and configuration of enterprise components using Mulesoft ESB. \nImplement enterprise grade platform (mark logic) for ETL from mainframe to NO SQL (cassandra).\nExperience on BI reporting with At Scale OLAP for Big Data. \nResponsible for importing log files from various sources into HDFS using Flume \nWorked on tools Flume, Storm and Spark. \nExpert in performing business analytical scripts using Hive SQL. \nBest practices for designing integration modules using ESB& Data Integrator modules    \nImplemented continuous integration & deployment (CICD) through Jenkins for Hadoop jobs. \nWorked in writing Hadoop Jobs for analyzing data using Hive, Pig accessing Text format files, sequence files, Parquet files.\nExperience in different Hadoop distributions like Cloudera (CDH3 & CDH4) and Horton Works Distributions (HDP) and MapR.\nExperience in integrating oozie logs to kibana dashboard. \nExtracted the data from MySQL, AWS RedShift into HDFS using Sqoop. \nDeveloped Spark code using Scala and Spark-SQL for faster testing and data processing. \nImported millions of structured data from relational databases using Sqoop import to process using Spark and stored the data into HDFS in CSV format. \nDeveloped Spark streaming application to pull data from cloud to Hive table.\nUsed Spark SQL to process the huge amount of structured data.\nAssigned name to each of the columns using case class option in Scala. \nImplemented Spark GraphX application to analyze guest behavior for data science segments.\nEnhancements to traditional data warehouse based on STAR schema, update data models, perform Data Analytics and Reporting using Tableau.\n\nEnvironment: Big Data, SparkSpark, YARN, HIVE, Pig, Scala, Python, Hadoop, AWS, Dynamo DB, Kibana, Cloudera, EMR, JDBC, Redshift, NOSQL, Sqoop, MYSQL.\n\nBNFS,Fort Worth, TX\t\t\t\t\t\t\t                                 Apr’13 – Jul’15\nSr. Java/ Hadoop Developer\n\nRoles & Responsibilities:\n\nInvolved in Big Data Project Implementation and Support. \nInvolved in the coding and integration of several business critical modules of CARE application using spring, Hibernate and REST web services on Web Sphere application server.\nImplemented Installation and configuration of multi-node cluster on Cloud using AWS on EC2.\nDesigned and developed Enterprise Eligibility business objects and domain objects with Object Relational Mapping framework such as Hibernate.\nUsed Hive to analyze data ingested into HBase by using Hive-HBase integration and compute various metrics for reporting on the dashboard\nDeveloped the Web Based Rich Internet Application (RIA) using JAVA/J2EE (spring framework).\nUsed the light weight container of the Spring Frame work to provide architectural flexibility for inversion of controller (IOC).\nUtilized Oozie workflow to run Pig and Hive Jobs Extracted files from Mongo DB through Sqoop and placed in HDFS and processed.\nUsed Flume to collect, aggregate, and store the web log data from different sources like web servers, mobile and network devices and pushed to HDFS.\nInvolved in end to end implementation of Big data design.\nDeveloped and Implemented new UI's using Angular JS and HTML.\nDeveloped Spring Configuration for dependency injection by using Spring IOC, Spring Controllers.\nAll the data was loaded from our relational DBs to HIVE using Sqoop. We were getting four flat files from different vendors. These were all in different formats e.g. text, EDI and XML formats\nObjective of this project is to build a data lake as a cloud based solution in AWS using Apache Spark and provide visualization of the ETL orchestration using CDAP tool.\nProof-of-concept to determine feasibility and product evaluation of Big Data products \nWriting Hive join query to fetch info from multiple tables, writing multiple Map Reduce jobs to collect output from Hive\nUsed Hive to analyze the partitioned and bucketed data and compute various metrics for reporting on the dashboard.\nAWS Cloud and On-Premise environments with Infrastructure Provisioning / Configuration.\nWorked on writing Perl scripts covering data feed handling, implementingmark logic, communicating with web-services through SOAP Lite module and WSDL. \nInvolved in developing Map-reduce framework, writing queries scheduling map-reduce\nDeveloped the code for Importing and exporting data into HDFS and Hive using Sqoop\nInstalled and configured Hadoop and responsible for maintaining cluster and managing and reviewing Hadoop log files.\nDeveloped Shell, Perl and Python scripts to automate and provide Control flow to Pig scripts.\nDesign of Redshift Data model, Redshift Performance improvements/analysis\nContinuous monitoring and managing the Hadoop cluster through Cloudera Manager.\nWorked on configuring and managing disaster recovery and backup on Cassandra Data.\nPerformed File system management and monitoring on Hadoop log files.\nImplemented partitioning, dynamic partitions and buckets in HIVE.\nDeveloped customized classes for serialization and Deserialization in Hadoop\nAnalyzed large amounts of data sets to determine optimal way to aggregate and report on it.\nImplemented a proof of concept deploying this product in Amazon Web Services AWS. \nInvolved in migration of data from existing RDBMS (oracle and SQL server) to Hadoop using Sqoop for processing data.\nEnvironment: Pig, Sqoop, Kafka, Apache Cassandra, Oozie, Impala, Cloudera, AWS, AWS EMR, Redshift, Flume, Apache Hadoop, HDFS, Hive, Map Reduce, Cassandra, Zookeeper, MySQL, Eclipse, Dynamo DB, PL/SQL and Python.\n\nWW Norton, NYC, NY                                                                                                                Nov’11 – Mar’13\nJAVA/J2EE DEVELOPER\n\nRoles & Responsibilities:\n\nUsed Web Logic to build and deploy the application. \nCreated stubs to consume Web services.\nAutomated tests were coded in Java Script with Frog logic’s Squish or Smart Bear’s Test Complete for client applications and coded in Java with Selenium for web application testing.\nDeveloped automation testing process using Selenium and QTP which involves study of client testing requirements, analyzing the feasible testing strategies and development of automated test scripts which also includes testing and finally deployment of the test scripts. \nUsed spring framework to achieve loose coupling between the layers thus moving towards Service Oriented Architecture (SOA) exposed through RESTful.  \nInvolved in performing Unit and Integration testing (Junit) \nInvolved in building EJB Session/Entity beans to maintain Transaction Management across the application.\nBuilt Web pages that is more user-interactive using Java script and Angular JS.\nGroovy allows using the primitive’s types as a short form for the variable declaration and the compiler translates this into the object.\nExtensively used Spring JDBC in data access layer to access and update information in the database.\nDeveloped Web Services to create reports module and send it to different agencies and premium calculation for manual classes using SOAP and Restful web services and rich faces components.\nInvolved in writing Spring MVC controllers and writing custom validations. \nWorking on Struts Framework for developing the front-end application and extensively. Spring as middle tier for entire application. \nUsed JAX-WS (SOAP) and JAX-RS (REST) to produce web services and involved in writing programs to consume the web services.\nJava 8, J2EE, spring (MVC, Data-JPA, Security),Hibernate, Jenkins or Bamboo, HTML 5, JSP, JavaScript, JQuery, Ajax, Angular JS\nInvolved in working with Struts Tiles for the common look and feel for a web application. \nWorking on Web Services using Java API for XML Services (JAX-WS) and supporting, building, deploying Web APIs Services. \nWorking as a part of team from business transfer, development, testing, code review, build implementation and support. \nWrote PL/SQL statements according to the need using Oracle 10g database. \nWorking on an internal web-based client server application built with Struts 2 Framework using Oracle backend Database, working on establishing the relation for the different beans using the Hibernate 3.1. \nInvolved in writing various components using Spring AOP and IOC framework.\nInvolved in writing JSP and JSF components. Used JSTL Tag library (Core, Logic, Nested, Beans and Html tag lib's) to create standard dynamic web pages.\nDeveloped connection to the backend using JDBC after building the Entity Beans as Bean Managed Persistence Entity Beans.\nDesigned and Developed the UI Framework using Spring MVC and AngularJS.\nCreation of REST Web Services for the management of data using Apache CXF and Docker.\nImplementation of EJB as entry point for web services. Effectively prepared for and organized technical inspections to review code and design models with peers and software architects. \nIdentified the defects through Selenium and ensured that business processes deliver the expected results and remain reliable throughout the production release.\nSpring 3.x is used as framework to write the application code and RESTful web services for external clients. \nDesigned and developed backend application servers using Python.\nManaged application deployment using Python.\nUpgraded Python 2.3 to Python 2.5, this required recompiling mode Python to use Python 2.5.\nEnhanced user experience by designing new web features using MVC Framework like Backbone.js, and node.js.\nUsed JDBC connectivity for connecting to the Oracle 8.0 database.\nDeveloped major websites and services by including Mongo DB as backend software.\nGood experience in creating and consuming Restful and SOAP Web Services.  \nDeveloping ability to move and consolidate critical information for the businesses and financial account data Using EJB 2.1 and Hibernate for performing the Database Transactions. \nEnvironment: Java and , Struts Framework ,J query, Oracle , HTML, Mark logic, micro services, Python, Groovy,  PL/SQL, JDBC, Mark logic, Talend, Hibernate, Ant, WSDL, EJB .\n\nInfinite Computer Solutions, BANGALORE                                                                              Jun’08 – Oct’11\t                                                                                         \nJAVA DEVELOPER\n\nRoles & Responsibilities:\n\nInvolved in various phases of Software Development Life Cycle (SDLC) of the application like Requirement gathering, Design, Analysis and Code development.\nDeveloped hibernate mapping using db model. \nInvolved in designing and developing Customized tags using JSP tag lib \nImplemented Model View Control (MVC) architecture using Struts Framework and Spring framework \nDeveloped browser-based Java Server Faces front-end to an AS/400 system \nUsed Ajax to provide dynamic features where applicable.\nImplemented RESTful web services to communicate with components of other Sourcing systems within the firm and to provide data to the reporting team. \nUsed MVC pattern for GUI development in JSF and worked closely with JSF lifecycle, Servlets and JSPs are used for real-time reporting which is too complex to be handled by the Business Objects \nUsed Jira for bug tracking and project management.\nPrepared user documentation with screenshots for UAT (User Acceptance testing). \nImplemented Struts Validation Framework for Server side validation. \nDeveloped JSP's with Custom Tag Libraries for control of the business processes in the middle-tier and was involved in their integration. \nDeveloped Web services (SOAP) through WSDL in Apache Axis to interact with other components. \nImplemented EJBs Session beans for business logic. \nUsed parsers like SAX and DOM for parsing xml documents and used XML transformations using XSLT. \nWrote stored procedures, triggers, and cursors using Oracle PL/SQL. \nUsed Rational Clear Case as Version control. \nImplemented Java/J2EE Design patterns like Business Delegate and Data Transfer Object (DTO), Data Access Object and Service Locator. \nInteract with clients to understand their needs and propose design to the team to implement the requirement.\nBuilt an online system using XML, Java script, AJAX, Strut 2.0, JDBC \nInvolved in technical Documentation for the module \nDesigned and created SQL Server Database, Stored Procedures \n \nEnvironment: Java, JSP, JDBC, Cassandra , API , Python, J query, Angular JS along with Web service , REST , Spring Core, Struts, Hibernate, Design Patterns, XML, Oracle, Apache Axis, ANT, Junit, UML, Web services, SOAP, XSLT, Jira.\n",
    "extracted_skills": [
        "agile",
        "angular",
        "aws",
        "azure",
        "cassandra",
        "css",
        "data analysis",
        "docker",
        "ec2",
        "hadoop",
        "html",
        "java",
        "javascript",
        "jenkins",
        "jira",
        "lambda",
        "machine learning",
        "mongodb",
        "mysql",
        "node.js",
        "oracle",
        "project management",
        "python",
        "r",
        "rds",
        "s3",
        "scala",
        "spark",
        "sql",
        "tableau"
    ]
}