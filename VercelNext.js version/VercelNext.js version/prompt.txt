Prompt 1: Data Acquisition and Preprocessing

ROLE: You are a data engineer tasked with setting up the initial data pipelines for an NLP project. Your goal is to create robust Python scripts to download and preprocess data from various sources.

PROJECT CONTEXT: We are building a resume skill gap analyzer. This first step involves collecting three types of data: a large corpus of resumes and job descriptions for model training, and a catalog of online courses for the recommendation engine.

TASK:

Create a Python script named data_collector.py. This script should contain separate functions to perform the following tasks:

Setup:

Create a main directory named project_data. Inside it, create three subdirectories: resumes, job_descriptions, and courses.

Include a function to download and extract datasets using libraries like requests, zipfile, and the Kaggle API. Ensure functions handle potential download errors gracefully.

Resume Data Collection:

Implement a function fetch_resume_datasets() that downloads and processes the following publicly available resume datasets into a unified text format:

Resume Dataset (Kaggle):  Download from    

snehaanbhawal/resume-dataset. Extract the Resume_str column from the CSV and save each resume as a separate .txt file in the project_data/resumes/raw_text/ directory.

Dataturks Resume Entities (Kaggle):  Download from    

dataturks/resume-entities-for-ner. This is an annotated dataset. Parse the JSON file to extract the raw text content of each resume. Save each as a .txt file in project_data/resumes/raw_text/. Also, create a separate function to parse the annotations into a pandas DataFrame (columns: resume_id, entity_text, entity_label, start_pos, end_pos) and save it as annotated_resumes.csv in project_data/resumes/.

Job Description Data Collection:

Implement a function fetch_job_description_datasets() that downloads and processes job description data:

Job Descriptions Dataset (Kaggle):  Download from    

kshitizregmi/jobs-and-job-description. Extract the Job Description column and save each as a .txt file in the project_data/job_descriptions/ directory.

Course Data Collection:

Implement a function fetch_course_datasets() that downloads course catalog data:

Online Courses (Kaggle):  Download from    

khaledatef1/online-courses.

Multi-platform Online Courses (Kaggle):  Download from    

everydaycodings/multi-platform-online-courses-dataset.

Create a data cleaning function that merges these datasets, standardizes column names (e.g., course_title, course_description, skills_taught, platform), handles missing values, and saves the final clean catalog as course_catalog.csv in the project_data/courses/ directory.

Main Execution Block:

Use a if __name__ == "__main__": block to call these functions sequentially to execute the data collection process. Add print statements to indicate progress.
